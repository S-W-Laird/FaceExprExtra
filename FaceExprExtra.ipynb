{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据，或 https://www.kaggle.com/datasets/cihan063/autism-image-data/data 直接下载 zip 文件\n",
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"cihan063/autism-image-data\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/PathToYourData/AutismDataset/consolidated'\n",
    "class_names = ['Autistic', 'Non_Autistic']\n",
    "\n",
    "organized_dir = '/PathToYourData/AutismDataset/split_data'\n",
    "os.makedirs(organized_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(organized_dir, 'train')\n",
    "val_dir = os.path.join(organized_dir, 'val')\n",
    "test_dir = os.path.join(organized_dir, 'test')\n",
    "\n",
    "for split_dir in [train_dir, val_dir, test_dir]:\n",
    "    for class_name in class_names:\n",
    "        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
    "\n",
    "def organize_class_images(src_class_dir, dest_train_dir, dest_val_dir, dest_test_dir, test_size=0.2, val_size=0.2):\n",
    "    image_files = [f for f in os.listdir(src_class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    train_files, test_files = train_test_split(image_files, test_size=test_size, random_state=42)\n",
    "    train_files, val_files = train_test_split(train_files, test_size=val_size, random_state=42)\n",
    "    \n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(src_class_dir, file), os.path.join(dest_train_dir, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(src_class_dir, file), os.path.join(dest_val_dir, file))\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(src_class_dir, file), os.path.join(dest_test_dir, file))\n",
    "\n",
    "for class_name in class_names:\n",
    "    src_class_dir = os.path.join(data_dir, class_name)\n",
    "    dest_train_dir = os.path.join(train_dir, class_name)\n",
    "    dest_val_dir = os.path.join(val_dir, class_name)\n",
    "    dest_test_dir = os.path.join(test_dir, class_name)\n",
    "    \n",
    "    organize_class_images(src_class_dir, dest_train_dir, dest_val_dir, dest_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "data_dir = '/data7/cyd/files/data/AutismDataset/split_data'\n",
    "\n",
    "vit_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=vit_transform)\n",
    "val_dataset = ImageFolder(os.path.join(data_dir, 'val'), transform=vit_transform)\n",
    "test_dataset = ImageFolder(os.path.join(data_dir, 'test'), transform=vit_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为训练集提取特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 59/59 [01:51<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已保存到 /data7/cyd/files/data/AutismDataset/vit_large_features/train\n",
      "特征形状: (1880, 1024), 标签形状: (1880,)\n",
      "\n",
      "为验证集提取特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 15/15 [00:03<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已保存到 /data7/cyd/files/data/AutismDataset/vit_large_features/val\n",
      "特征形状: (472, 1024), 标签形状: (472,)\n",
      "\n",
      "为测试集提取特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 19/19 [00:04<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已保存到 /data7/cyd/files/data/AutismDataset/vit_large_features/test\n",
      "特征形状: (588, 1024), 标签形状: (588,)\n",
      "\n",
      "特征提取完成!\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_name='vit_large_patch16_224', ckpt_path=None):\n",
    "        super(ViTFeatureExtractor, self).__init__()\n",
    "        # self.vit = timm.create_model(model_name, pretrained=False, num_classes=0)  # 不自动下载\n",
    "        self.vit = timm.create_model(model_name, pretrained=True)\n",
    "        if ckpt_path is not None:\n",
    "            state_dict = load_file(ckpt_path)  # 用 safetensors 读取\n",
    "            self.vit.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False     # 冻结所有参数\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.vit(x)\n",
    "\n",
    "\n",
    "model = ViTFeatureExtractor(\n",
    "    model_name=\"vit_large_patch16_224\",\n",
    "    # model_name=\"vit_large_patch16_224.augreg_in21k_ft_in1k\",\n",
    "    # ckpt_path = \"/data7/cyd/.cache/huggingface/hub/models--timm--vit_large_patch16_224.augreg_in21k_ft_in1k/snapshots/0930ab3308b84cb2ae091a4a80703c459412a4c7/model.safetensors\"\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "def extract_and_save_features(data_loader, dataset, save_path):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc='提取特征'):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            \n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "            \n",
    "            batch_filenames = [dataset.samples[i][0] for i in range(len(labels))]\n",
    "            all_filenames.extend(batch_filenames)\n",
    "    \n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    np.save(os.path.join(save_path, 'features.npy'), all_features)\n",
    "    np.save(os.path.join(save_path, 'labels.npy'), all_labels)\n",
    "    \n",
    "    with open(os.path.join(save_path, 'filenames.txt'), 'w') as f:\n",
    "        for filename in all_filenames:\n",
    "            f.write(f\"{filename}\\n\")\n",
    "    \n",
    "    print(f\"特征已保存到 {save_path}\")\n",
    "    print(f\"特征形状: {all_features.shape}, 标签形状: {all_labels.shape}\")\n",
    "\n",
    "print(\"为训练集提取特征...\")\n",
    "extract_and_save_features(train_loader, train_dataset, '/PathToYourData/AutismDataset/vit_large_features/train')\n",
    "\n",
    "print(\"\\n为验证集提取特征...\")\n",
    "extract_and_save_features(val_loader, val_dataset, '/PathToYourData/AutismDataset/vit_large_features/val')\n",
    "\n",
    "print(\"\\n为测试集提取特征...\")\n",
    "extract_and_save_features(test_loader, test_dataset, '/PathToYourData/AutismDataset/vit_large_features/test')\n",
    "\n",
    "print(\"\\n特征提取完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练特征形状: (1880, 1024), 训练标签形状: (1880,)\n",
      "测试特征形状: (588, 1024), 测试标签形状: (588,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def load_features(feature_dir):\n",
    "    features = np.load(os.path.join(feature_dir, 'features.npy'))\n",
    "    labels = np.load(os.path.join(feature_dir, 'labels.npy'))\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = load_features('/data7/cyd/files/data/AutismDataset/vit_large_features/train')\n",
    "val_features, val_labels = load_features('/data7/cyd/files/data/AutismDataset/vit_large_features/val')\n",
    "test_features, test_labels = load_features('/data7/cyd/files/data/AutismDataset/vit_large_features/test')\n",
    "\n",
    "# 合并训练和验证集\n",
    "X = np.concatenate([train_features, val_features])\n",
    "y = np.concatenate([train_labels, val_labels])\n",
    "\n",
    "print(f\"训练特征形状: {train_features.shape}, 训练标签形状: {train_labels.shape}\")\n",
    "print(f\"测试特征形状: {test_features.shape}, 测试标签形状: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vit_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: LR=8.00e-05\n",
      "Train Loss: 1.1465 | Acc: 0.5307\n",
      "Val Loss: 0.6296 | Acc: 0.6208\n",
      "↳ 保存最佳模型\n",
      "Epoch 2/50: LR=1.20e-04\n",
      "Train Loss: 0.9270 | Acc: 0.5884\n",
      "Val Loss: 0.5391 | Acc: 0.7352\n",
      "↳ 保存最佳模型\n",
      "Epoch 3/50: LR=1.60e-04\n",
      "Train Loss: 0.7448 | Acc: 0.6853\n",
      "Val Loss: 0.5028 | Acc: 0.7754\n",
      "↳ 保存最佳模型\n",
      "Epoch 4/50: LR=2.00e-04\n",
      "Train Loss: 0.5955 | Acc: 0.7645\n",
      "Val Loss: 0.4839 | Acc: 0.8008\n",
      "↳ 保存最佳模型\n",
      "Epoch 5/50: LR=2.00e-04\n",
      "Train Loss: 0.5332 | Acc: 0.8017\n",
      "Val Loss: 0.4622 | Acc: 0.8284\n",
      "↳ 保存最佳模型\n",
      "Epoch 6/50: LR=2.00e-04\n",
      "Train Loss: 0.4819 | Acc: 0.8244\n",
      "Val Loss: 0.4479 | Acc: 0.8475\n",
      "↳ 保存最佳模型\n",
      "Epoch 7/50: LR=2.00e-04\n",
      "Train Loss: 0.4392 | Acc: 0.8561\n",
      "Val Loss: 0.4498 | Acc: 0.8411\n",
      "↳ 保存最佳模型\n",
      "Epoch 8/50: LR=2.00e-04\n",
      "Train Loss: 0.4234 | Acc: 0.8745\n",
      "Val Loss: 0.4445 | Acc: 0.8475\n",
      "↳ 保存最佳模型\n",
      "Epoch 9/50: LR=2.00e-04\n",
      "Train Loss: 0.4121 | Acc: 0.8879\n",
      "Val Loss: 0.4496 | Acc: 0.8390\n",
      "↳ 保存最佳模型\n",
      "Epoch 10/50: LR=2.00e-04\n",
      "Train Loss: 0.3638 | Acc: 0.9116\n",
      "Val Loss: 0.4447 | Acc: 0.8496\n",
      "↳ 保存最佳模型\n",
      "Epoch 11/50: LR=2.00e-04\n",
      "Train Loss: 0.3549 | Acc: 0.9116\n",
      "Val Loss: 0.4417 | Acc: 0.8475\n",
      "↳ 保存最佳模型\n",
      "Epoch 12/50: LR=2.00e-04\n",
      "Train Loss: 0.3404 | Acc: 0.9332\n",
      "Val Loss: 0.4396 | Acc: 0.8517\n",
      "↳ 保存最佳模型\n",
      "Epoch 13/50: LR=2.00e-04\n",
      "Train Loss: 0.3241 | Acc: 0.9407\n",
      "Val Loss: 0.4512 | Acc: 0.8475\n",
      "↳ 保存最佳模型\n",
      "Epoch 14/50: LR=2.00e-04\n",
      "Train Loss: 0.3223 | Acc: 0.9413\n",
      "Val Loss: 0.4486 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 15/50: LR=2.00e-04\n",
      "Train Loss: 0.3118 | Acc: 0.9499\n",
      "Val Loss: 0.4456 | Acc: 0.8538\n",
      "↳ 保存最佳模型\n",
      "Epoch 16/50: LR=2.00e-04\n",
      "Train Loss: 0.3110 | Acc: 0.9499\n",
      "Val Loss: 0.4315 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 17/50: LR=2.00e-04\n",
      "Train Loss: 0.2985 | Acc: 0.9612\n",
      "Val Loss: 0.4515 | Acc: 0.8538\n",
      "↳ 保存最佳模型\n",
      "Epoch 18/50: LR=2.00e-04\n",
      "Train Loss: 0.2854 | Acc: 0.9688\n",
      "Val Loss: 0.4488 | Acc: 0.8623\n",
      "↳ 保存最佳模型\n",
      "Epoch 19/50: LR=2.00e-04\n",
      "Train Loss: 0.2897 | Acc: 0.9639\n",
      "Val Loss: 0.4387 | Acc: 0.8623\n",
      "↳ 保存最佳模型\n",
      "Epoch 20/50: LR=2.00e-04\n",
      "Train Loss: 0.2815 | Acc: 0.9720\n",
      "Val Loss: 0.4468 | Acc: 0.8453\n",
      "↳ 保存最佳模型\n",
      "Epoch 21/50: LR=1.60e-04\n",
      "Train Loss: 0.2799 | Acc: 0.9704\n",
      "Val Loss: 0.4410 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 22/50: LR=1.20e-04\n",
      "Train Loss: 0.2709 | Acc: 0.9763\n",
      "Val Loss: 0.4440 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 23/50: LR=8.00e-05\n",
      "Train Loss: 0.2682 | Acc: 0.9774\n",
      "Val Loss: 0.4371 | Acc: 0.8517\n",
      "↳ 保存最佳模型\n",
      "Epoch 24/50: LR=4.00e-05\n",
      "Train Loss: 0.2681 | Acc: 0.9790\n",
      "Val Loss: 0.4345 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 25/50: LR=0.00e+00\n",
      "Train Loss: 0.2670 | Acc: 0.9806\n",
      "Val Loss: 0.4432 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 26/50: LR=0.00e+00\n",
      "Train Loss: 0.2658 | Acc: 0.9801\n",
      "Val Loss: 0.4343 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 27/50: LR=0.00e+00\n",
      "Train Loss: 0.2591 | Acc: 0.9838\n",
      "Val Loss: 0.4330 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 28/50: LR=0.00e+00\n",
      "Train Loss: 0.2644 | Acc: 0.9758\n",
      "Val Loss: 0.4361 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 29/50: LR=0.00e+00\n",
      "Train Loss: 0.2630 | Acc: 0.9795\n",
      "Val Loss: 0.4362 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 30/50: LR=0.00e+00\n",
      "Train Loss: 0.2636 | Acc: 0.9881\n",
      "Val Loss: 0.4374 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 31/50: LR=0.00e+00\n",
      "Train Loss: 0.2640 | Acc: 0.9811\n",
      "Val Loss: 0.4416 | Acc: 0.8644\n",
      "↳ 保存最佳模型\n",
      "Epoch 32/50: LR=0.00e+00\n",
      "Train Loss: 0.2653 | Acc: 0.9811\n",
      "Val Loss: 0.4332 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 33/50: LR=0.00e+00\n",
      "Train Loss: 0.2675 | Acc: 0.9833\n",
      "Val Loss: 0.4356 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 34/50: LR=0.00e+00\n",
      "Train Loss: 0.2584 | Acc: 0.9860\n",
      "Val Loss: 0.4323 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 35/50: LR=0.00e+00\n",
      "Train Loss: 0.2624 | Acc: 0.9817\n",
      "Val Loss: 0.4350 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 36/50: LR=0.00e+00\n",
      "Train Loss: 0.2595 | Acc: 0.9849\n",
      "Val Loss: 0.4383 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 37/50: LR=0.00e+00\n",
      "Train Loss: 0.2678 | Acc: 0.9784\n",
      "Val Loss: 0.4415 | Acc: 0.8538\n",
      "↳ 保存最佳模型\n",
      "Epoch 38/50: LR=0.00e+00\n",
      "Train Loss: 0.2645 | Acc: 0.9801\n",
      "Val Loss: 0.4403 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 39/50: LR=0.00e+00\n",
      "Train Loss: 0.2680 | Acc: 0.9763\n",
      "Val Loss: 0.4369 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 40/50: LR=0.00e+00\n",
      "Train Loss: 0.2661 | Acc: 0.9811\n",
      "Val Loss: 0.4384 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 41/50: LR=0.00e+00\n",
      "Train Loss: 0.2728 | Acc: 0.9741\n",
      "Val Loss: 0.4422 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 42/50: LR=0.00e+00\n",
      "Train Loss: 0.2625 | Acc: 0.9795\n",
      "Val Loss: 0.4407 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 43/50: LR=0.00e+00\n",
      "Train Loss: 0.2646 | Acc: 0.9758\n",
      "Val Loss: 0.4334 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 44/50: LR=0.00e+00\n",
      "Train Loss: 0.2651 | Acc: 0.9811\n",
      "Val Loss: 0.4397 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 45/50: LR=0.00e+00\n",
      "Train Loss: 0.2611 | Acc: 0.9844\n",
      "Val Loss: 0.4431 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "Epoch 46/50: LR=0.00e+00\n",
      "Train Loss: 0.2642 | Acc: 0.9795\n",
      "Val Loss: 0.4376 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 47/50: LR=0.00e+00\n",
      "Train Loss: 0.2630 | Acc: 0.9849\n",
      "Val Loss: 0.4337 | Acc: 0.8602\n",
      "↳ 保存最佳模型\n",
      "Epoch 48/50: LR=0.00e+00\n",
      "Train Loss: 0.2627 | Acc: 0.9822\n",
      "Val Loss: 0.4373 | Acc: 0.8559\n",
      "↳ 保存最佳模型\n",
      "Epoch 49/50: LR=0.00e+00\n",
      "Train Loss: 0.2627 | Acc: 0.9806\n",
      "Val Loss: 0.4380 | Acc: 0.8623\n",
      "↳ 保存最佳模型\n",
      "Epoch 50/50: LR=0.00e+00\n",
      "Train Loss: 0.2658 | Acc: 0.9828\n",
      "Val Loss: 0.4370 | Acc: 0.8581\n",
      "↳ 保存最佳模型\n",
      "\n",
      "在测试集上评估最佳模型...\n",
      "\n",
      "测试集性能:\n",
      "- 准确率: 0.8724\n",
      "- 交叉熵损失: 0.4248\n"
     ]
    }
   ],
   "source": [
    "def create_data_loaders(train_features, train_labels, val_features, val_labels, test_features, test_labels, batch_size=64):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(train_features), \n",
    "        torch.LongTensor(train_labels)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.FloatTensor(val_features), \n",
    "        torch.LongTensor(val_labels)\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.FloatTensor(test_features), \n",
    "        torch.LongTensor(test_labels)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    train_features, train_labels,\n",
    "    val_features, val_labels,\n",
    "    test_features, test_labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# 分类器\n",
    "class ViTLargeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1024, num_classes=2):\n",
    "        super(ViTLargeClassifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 768),\n",
    "            nn.BatchNorm1d(768),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(384, 192),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(192, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 权重初始化\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, device, optimizer, criterion, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_metric = 0.0\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def evaluate(self, data_loader, return_predictions=False):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        if return_predictions:\n",
    "            return epoch_loss, epoch_acc, all_labels, all_preds, all_probs\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=30, early_stop_patience=5):\n",
    "        best_model_wts = None\n",
    "        no_improve = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            val_loss, val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_acc)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            \n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}: LR={lr:.2e}')\n",
    "            print(f'Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}')\n",
    "            print(f'Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # if val_acc > self.best_metric:\n",
    "            self.best_metric = val_acc\n",
    "            best_model_wts = self.model.state_dict()\n",
    "            torch.save(best_model_wts, 'best_classifier.pth')\n",
    "            print('↳ 保存最佳模型')\n",
    "            no_improve = 0\n",
    "            # else:\n",
    "            #     no_improve += 1\n",
    "            #     if no_improve >= early_stop_patience:\n",
    "            #         print(f'↳ 早停触发，在 {epoch+1} 个epoch后停止训练')\n",
    "            #         break\n",
    "        \n",
    "        if best_model_wts is not None:\n",
    "            self.model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "model = ViTLargeClassifier(input_dim=train_features.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 5:  # 前5个epoch线性warmup\n",
    "        return (epoch + 1) / 5\n",
    "    elif epoch < 20:  # 然后保持\n",
    "        return 1.0\n",
    "    else:  # 最后线性衰减\n",
    "        return max(0.0, (25 - epoch) / 5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# train\n",
    "trainer = Trainer(model, device, optimizer, criterion, scheduler)\n",
    "trainer.train(train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "# test\n",
    "print(\"\\n在测试集上评估最佳模型...\")\n",
    "test_loss, test_acc, y_true, y_pred, y_probs = trainer.evaluate(test_loader, return_predictions=True)\n",
    "\n",
    "print(f\"\\n测试集性能:\")\n",
    "print(f\"- 准确率: {test_acc:.4f}\")\n",
    "print(f\"- 交叉熵损失: {test_loss:.4f}\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_dim': train_features.shape[1],\n",
    "    'num_classes': 2\n",
    "}, 'final_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vit_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "训练简单分类器...\n",
      "Epoch 1/20:\n",
      "Train Loss: 0.4459 | Acc: 0.7851\n",
      "Val Loss: 0.4014 | Acc: 0.8030\n",
      "↳ 保存最佳模型\n",
      "Epoch 2/20:\n",
      "Train Loss: 0.2865 | Acc: 0.8824\n",
      "Val Loss: 0.3718 | Acc: 0.8199\n",
      "↳ 保存最佳模型\n",
      "Epoch 3/20:\n",
      "Train Loss: 0.2267 | Acc: 0.9043\n",
      "Val Loss: 0.4453 | Acc: 0.8008\n",
      "Epoch 4/20:\n",
      "Train Loss: 0.1794 | Acc: 0.9378\n",
      "Val Loss: 0.3919 | Acc: 0.8326\n",
      "↳ 保存最佳模型\n",
      "Epoch 5/20:\n",
      "Train Loss: 0.1409 | Acc: 0.9590\n",
      "Val Loss: 0.4108 | Acc: 0.8242\n",
      "Epoch 6/20:\n",
      "Train Loss: 0.1156 | Acc: 0.9633\n",
      "Val Loss: 0.4343 | Acc: 0.8157\n",
      "Epoch 7/20:\n",
      "Train Loss: 0.0838 | Acc: 0.9787\n",
      "Val Loss: 0.4776 | Acc: 0.8284\n",
      "Epoch 8/20:\n",
      "Train Loss: 0.0935 | Acc: 0.9691\n",
      "Val Loss: 0.4149 | Acc: 0.8114\n",
      "Epoch 9/20:\n",
      "Train Loss: 0.0488 | Acc: 0.9910\n",
      "Val Loss: 0.4434 | Acc: 0.8220\n",
      "Epoch 10/20:\n",
      "Train Loss: 0.0427 | Acc: 0.9926\n",
      "Val Loss: 0.4326 | Acc: 0.8263\n",
      "Epoch 11/20:\n",
      "Train Loss: 0.0364 | Acc: 0.9936\n",
      "Val Loss: 0.4534 | Acc: 0.8326\n",
      "Epoch 12/20:\n",
      "Train Loss: 0.0321 | Acc: 0.9957\n",
      "Val Loss: 0.4623 | Acc: 0.8347\n",
      "↳ 保存最佳模型\n",
      "Epoch 13/20:\n",
      "Train Loss: 0.0293 | Acc: 0.9973\n",
      "Val Loss: 0.4713 | Acc: 0.8326\n",
      "Epoch 14/20:\n",
      "Train Loss: 0.0231 | Acc: 0.9989\n",
      "Val Loss: 0.4909 | Acc: 0.8220\n",
      "Epoch 15/20:\n",
      "Train Loss: 0.0214 | Acc: 0.9989\n",
      "Val Loss: 0.5072 | Acc: 0.8347\n",
      "Epoch 16/20:\n",
      "Train Loss: 0.0178 | Acc: 1.0000\n",
      "Val Loss: 0.4922 | Acc: 0.8390\n",
      "↳ 保存最佳模型\n",
      "Epoch 17/20:\n",
      "Train Loss: 0.0143 | Acc: 0.9989\n",
      "Val Loss: 0.4972 | Acc: 0.8305\n",
      "Epoch 18/20:\n",
      "Train Loss: 0.0141 | Acc: 0.9989\n",
      "Val Loss: 0.5095 | Acc: 0.8411\n",
      "↳ 保存最佳模型\n",
      "Epoch 19/20:\n",
      "Train Loss: 0.0158 | Acc: 0.9979\n",
      "Val Loss: 0.5047 | Acc: 0.8453\n",
      "↳ 保存最佳模型\n",
      "Epoch 20/20:\n",
      "Train Loss: 0.0137 | Acc: 0.9995\n",
      "Val Loss: 0.5078 | Acc: 0.8326\n",
      "\n",
      "测试集性能 - 准确率: 0.8282\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "train_features_tensor = torch.FloatTensor(train_features)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "val_features_tensor = torch.FloatTensor(val_features)\n",
    "val_labels_tensor = torch.LongTensor(val_labels)\n",
    "test_features_tensor = torch.FloatTensor(test_features)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_features_tensor, val_labels_tensor)\n",
    "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# 定义简单分类器\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_classes=2):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleClassifier(input_dim=train_features.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=3, factor=0.5)\n",
    "\n",
    "# 训练函数\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "        \n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / len(train_dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_simple_classifier.pth')\n",
    "            print('↳ 保存最佳模型')\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in data_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    loss /= len(data_loader)\n",
    "    acc = correct / len(data_loader.dataset)\n",
    "    return loss, acc\n",
    "\n",
    "# 训练和评估\n",
    "print(\"\\n训练简单分类器...\")\n",
    "train_model(model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "# 加载最佳模型并测试\n",
    "model.load_state_dict(torch.load('best_simple_classifier.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"\\n测试集性能 - 准确率: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
