{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码功能：训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n",
    "需要先运行 data_organize.ipynb 组织数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data7/cyd/files/data/AutismDataset/split_data'\n",
    "\n",
    "vit_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=vit_transform)\n",
    "val_dataset = ImageFolder(os.path.join(data_dir, 'val'), transform=vit_transform)\n",
    "test_dataset = ImageFolder(os.path.join(data_dir, 'test'), transform=vit_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract feature\n",
    "使用 ViT 提取图像特征并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.feature_extractor import ViTFeatureExtractor\n",
    "from utils import extract_and_save_features\n",
    "\n",
    "# load model\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ViTFeatureExtractor(\n",
    "    model_name=\"vit_large_patch16_224\",\n",
    "    # model_name=\"vit_large_patch16_224.augreg_in21k_ft_in1k\",\n",
    "    # ckpt_path = \"/data7/cyd/.cache/huggingface/hub/models--timm--vit_large_patch16_224.augreg_in21k_ft_in1k/snapshots/0930ab3308b84cb2ae091a4a80703c459412a4c7/model.safetensors\"\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为训练集提取特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 59/59 [00:20<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已保存到 /data7/cyd/files/data/AutismDataset/vit_large_features/train\n",
      "特征形状: (1880, 1000), 标签形状: (1880,)\n",
      "\n",
      "为验证集提取特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 15/15 [00:03<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已保存到 /data7/cyd/files/data/AutismDataset/vit_large_features/val\n",
      "特征形状: (472, 1000), 标签形状: (472,)\n",
      "\n",
      "为测试集提取特征...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "提取特征: 100%|██████████| 19/19 [00:03<00:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已保存到 /data7/cyd/files/data/AutismDataset/vit_large_features/test\n",
      "特征形状: (588, 1000), 标签形状: (588,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"为训练集提取特征...\")\n",
    "extract_and_save_features(train_loader, model, train_dataset, '/data7/cyd/files/data/AutismDataset/vit_large_features/train', device)\n",
    "\n",
    "print(\"\\n为验证集提取特征...\")\n",
    "extract_and_save_features(val_loader, model, val_dataset, '/data7/cyd/files/data/AutismDataset/vit_large_features/val', device)\n",
    "\n",
    "print(\"\\n为测试集提取特征...\")\n",
    "extract_and_save_features(test_loader, model, test_dataset, '/data7/cyd/files/data/AutismDataset/vit_large_features/test', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load features  \n",
    "加载保存的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练特征形状: (1880, 1000), 训练标签形状: (1880,)\n",
      "测试特征形状: (588, 1000), 测试标签形状: (588,)\n"
     ]
    }
   ],
   "source": [
    "from utils import load_features\n",
    "\n",
    "train_features, train_labels = load_features('/data7/cyd/files/data/AutismDataset/vit_large_features/train')\n",
    "val_features, val_labels = load_features('/data7/cyd/files/data/AutismDataset/vit_large_features/val')\n",
    "test_features, test_labels = load_features('/data7/cyd/files/data/AutismDataset/vit_large_features/test')\n",
    "\n",
    "# 合并训练和验证集\n",
    "X = np.concatenate([train_features, val_features])\n",
    "y = np.concatenate([train_labels, val_labels])\n",
    "\n",
    "print(f\"训练特征形状: {train_features.shape}, 训练标签形状: {train_labels.shape}\")\n",
    "print(f\"测试特征形状: {test_features.shape}, 测试标签形状: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train  \n",
    "用 ViT 提取出的特征作为数据训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_data_loaders\n",
    "\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    train_features, train_labels,\n",
    "    val_features, val_labels,\n",
    "    test_features, test_labels,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbb826b8b8b4e6c81c510b5ecdc832a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Performance:\n",
      "- Accuracy: 0.8503\n",
      "- Loss: 0.4583\n"
     ]
    }
   ],
   "source": [
    "from models.classifier import ViTLargeClassifier\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, device, optimizer, criterion, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.best_metric = 0.0\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def evaluate(self, data_loader, return_predictions=False):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        if return_predictions:\n",
    "            return epoch_loss, epoch_acc, all_labels, all_preds, all_probs\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=30, early_stop_patience=5):\n",
    "        best_model_wts = None\n",
    "        no_improve = 0\n",
    "        \n",
    "        epoch_iter = tqdm(range(num_epochs), desc='Epochs', bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]')\n",
    "        for epoch in epoch_iter:\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            val_loss, val_acc = self.evaluate(val_loader)\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_acc)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            \n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            epoch_iter.set_postfix({\n",
    "                'lr': f'{lr:.2e}',\n",
    "                'train_loss': f'{train_loss:.4f}',\n",
    "                'train_acc': f'{train_acc:.4f}',\n",
    "                'val_loss': f'{val_loss:.4f}',\n",
    "                'val_acc': f'{val_acc:.4f}'\n",
    "            })\n",
    "            \n",
    "            self.best_metric = val_acc\n",
    "            best_model_wts = self.model.state_dict()\n",
    "            torch.save(best_model_wts, 'best_classifier.pth')\n",
    "            no_improve = 0\n",
    "        \n",
    "        if best_model_wts is not None:\n",
    "            self.model.load_state_dict(best_model_wts)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "model = ViTLargeClassifier(input_dim=train_features.shape[1]).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < 5:\n",
    "        return (epoch + 1) / 5\n",
    "    elif epoch < 20:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return max(0.0, (25 - epoch) / 5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# train\n",
    "trainer = Trainer(model, device, optimizer, criterion, scheduler)\n",
    "trainer.train(train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "# test\n",
    "test_loss, test_acc, y_true, y_pred, y_probs = trainer.evaluate(test_loader, return_predictions=True)\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"- Accuracy: {test_acc:.4f}\")\n",
    "print(f\"- Loss: {test_loss:.4f}\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_dim': train_features.shape[1],\n",
    "    'num_classes': 2\n",
    "}, 'final_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
